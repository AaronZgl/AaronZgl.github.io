<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[BeautifulSoup使用总结]]></title>
    <url>%2F2017%2F09%2F24%2FBeautifulSoup%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[Beautiful Soup 4.2.0 文档 1.功能网页解析器：从网页中提取有价值的数据 2.原理结构化解析：将HTML网页解析成DOM树（Document Object Model），以节点为单位进行查找。 3.安装 pip install beautifulsoup4 4.方法 find_all：寻找所有符合要求的元素 find：寻找第一个符合要求的元素 5.使用 1.创建BeautifulSoup对象 # 导入BeautifulSoup包 from bs4 import BeautifulSoup # 创建BeautifulSoup对象 soup = BeautifulSoup(html, &apos;html.parser&apos; from_encoding = &apos;utf-8&apos;) 2.搜索节点（find_all, find） # 寻找div节点 node = soup.find_all(&apos;div&apos;) # 寻找所有class属性为a的div节点 node = soup.find_all(&apos;div&apos;, class=&apos;a&apos;) # 寻找所有内容为p，class属性为a的div节点 node = soup.find_all(&apos;div&apos;, class=&apos;a&apos;, string=&apos;p&apos;) 3.访问节点信息 # 获得节点的标签名称 name = node.name # 获得节点的href属性 href = node[&apos;href&apos;] # 获得节点的文本内容 text = node.get_text() 6.实例 代码 # -*- coding: utf-8 -*- # 导入包 from bs4 import BeautifulSoup # html文档，来自官网文档 html_doc = &quot;&quot;&quot; &lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&apos;s story&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&apos;s story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were &lt;a href=&quot;http://example.com/elsie&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;http://example.com/lacie&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;http://example.com/tillie&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.&lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &quot;&quot;&quot; # 创建soup对象 soup = BeautifulSoup(html_doc, &apos;html.parser&apos; from_encoding = &apos;utf-8&apos;) # 寻找所有a节点 a1_nodes = soup.find_all(&apos;a&apos;) # 寻找id为‘link1’的a节点 a2_nodes = soup.find_all(&apos;a&apos;, id=&apos;link1&apos;) # 寻找class为story，内容为‘...’的节点 # class为python的保留字，所以这里下一个下斜杠使用class_ p_nodes = soup.find_all(&apos;p&apos;, class_=&apos;story&apos;, string=&apos;...&apos;) # 打印信息 for node in a1_nodes: print node[&apos;id&apos;] for node in a2_nodes: print node[&apos;href&apos;] for node in p_nodes: print node.get_text()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>BeautifulSoup</tag>
        <tag>python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F09%2F21%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
